{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Data for Decontextualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Get paragraphs (context) from the dataset. \n",
        "Format: SQuAD Format (https://github.com/rajpurkar/SQuAD-explorer/tree/master/dataset)\n",
        "\"\"\"\n",
        "import json\n",
        "data_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/dev-v1.1.json\"\n",
        "with open(data_path) as dataset_file:\n",
        "    dataset = json.load(dataset_file)['data']\n",
        "count = 0\n",
        "count_passage = 0\n",
        "new_dataset = []\n",
        "for passage in dataset:\n",
        "    count_passage += 1\n",
        "    for paragraph in passage['paragraphs']:\n",
        "        count += 1\n",
        "        new_paragraph = {}\n",
        "        new_paragraph['context'] = paragraph['context']\n",
        "        new_paragraph['id'] = str(count)\n",
        "        new_dataset.append(new_paragraph)\n",
        "to_save = {'data': new_dataset}\n",
        "json_object = json.dumps(to_save, indent=4)\n",
        "save_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/paragraph-dev-v1.1.json\"\n",
        "with open(save_path, \"w\") as outfile:\n",
        "    outfile.write(json_object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Divide the big paragraph file into smaller files for better it with checkpoints.\n",
        "\"\"\"\n",
        "import math\n",
        "import os\n",
        "paragraph_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/paragraph-dev-v1.1.json\"\n",
        "num_chunks = 20         # Divide the full SQuAD dev set into num_chunks for tracking the progress of decontextualization\n",
        "save_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/paragraph-dev-segment\"\n",
        "with open(paragraph_path) as dataset_file:\n",
        "    questions = json.load(dataset_file)['data']\n",
        "question_chunk = math.ceil(len(questions) / num_chunks)\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "for i in range(num_chunks):\n",
        "    new_dataset = {}\n",
        "    new_dataset['version'] = str(i+1)\n",
        "    start_index = i*question_chunk\n",
        "    end_index = min(len(questions), (i+1)*question_chunk)\n",
        "    new_dataset['data'] = questions[start_index:end_index]\n",
        "\n",
        "    #Save this\n",
        "    json_object = json.dumps(new_dataset, indent=4)\n",
        "    with open(save_path+ \"/\" +str(i+1) + \".json\", \"w\") as outfile:\n",
        "        outfile.write(json_object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decontextualization SQuAD\n",
        "\n",
        "For further information about Decontextualization, see [Decontextualization: Making Sentences Stand-Alone](https://arxiv.org/abs/2102.05169)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.13.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUGNUKuZ5Vky",
        "outputId": "ae2dad73-8308-4cc7-cfb8-a954cfbd81e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading SavedModel in eager mode.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-31 18:00:52.868764: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n",
            "2023-07-31 18:00:54.858563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-31 18:00:54.896953: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'shared/embedding:0' shape=(32128, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/layer_norm/scale:0' shape=(768,) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/q:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/k:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/v:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'shared/embedding:0' shape=(32128, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/layer_norm/scale:0' shape=(768,) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/q:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/k:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/v:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'shared/embedding:0' shape=(32128, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/layer_norm/scale:0' shape=(768,) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/q:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/k:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/v:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'shared/embedding:0' shape=(32128, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/layer_norm/scale:0' shape=(768,) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/q:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/k:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n",
            "WARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'encoder/block_000/layer_000/SelfAttention/v:0' shape=(768, 768) dtype=bfloat16_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\n"
          ]
        }
      ],
      "source": [
        "from os import path\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text  # Required to run exported model.\n",
        "\n",
        "MODEL_SIZE = \"base\" #@param[\"base\", \"3B\", \"11B\"]\n",
        "\n",
        "DATASET_BUCKET = 'gs://decontext_dataset'\n",
        "\n",
        "SAVED_MODELS = {\n",
        "  \"base\": f'{DATASET_BUCKET}/t5_base/1611267950',\n",
        "  \"3B\": f'{DATASET_BUCKET}/t5_3B/1611333896',\n",
        "  \"11B\": f'{DATASET_BUCKET}/t5_11B/1605298402'\n",
        "}\n",
        "\n",
        "SAVED_MODEL_PATH = SAVED_MODELS[MODEL_SIZE]\n",
        "DEV = path.join(DATASET_BUCKET, 'decontext_dev.jsonl')\n",
        "SAVED_MODEL_PATH = path.join(DATASET_BUCKET, 't5_base/1611267950')\n",
        "\n",
        "def load_predict_fn(model_path):\n",
        "  print(\"Loading SavedModel in eager mode.\")\n",
        "  imported = tf.saved_model.load(model_path, [\"serve\"])\n",
        "  return lambda x: imported.signatures['serving_default'](\n",
        "      tf.constant(x))['outputs'].numpy()\n",
        "\n",
        "predict_fn = load_predict_fn(SAVED_MODEL_PATH)\n",
        "\n",
        "def decontextualize(input):\n",
        "  return predict_fn([input])[0].decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtKw9bGzA_qO"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def segment_context(context):\n",
        "    doc = nlp(context)\n",
        "    sentences = [str(sentence) for sentence in doc.sents]\n",
        "    return sentences\n",
        "def create_input(paragraph, target_sentence_idx, page_title='', section_title=''):\n",
        "    \"\"\"Creates a single Decontextualization example input for T5.\n",
        "\n",
        "    Args:\n",
        "      paragraph: List of strings. Each string is a single sentence.\n",
        "      target_sentence_idx: Integer index into `paragraph` indicating which\n",
        "        sentence should be decontextualized.\n",
        "      page_title: Optional title string. Usually Wikipedia page title.\n",
        "      section_title: Optional title of section within page.\n",
        "    \"\"\"\n",
        "    prefix = ' '.join(paragraph[:target_sentence_idx])\n",
        "    target = paragraph[target_sentence_idx]\n",
        "    suffix = ' '.join(paragraph[target_sentence_idx + 1:])\n",
        "    return ' [SEP] '.join((page_title, section_title, prefix, target, suffix))\n",
        "def segment_dataset(data_path, save_path):\n",
        "    new_dataset = []\n",
        "    with open(data_path) as dataset_file:\n",
        "        dataset = json.load(dataset_file)['data']\n",
        "    for para in dataset:\n",
        "        sentences = segment_context(para['context'])\n",
        "\n",
        "        new_para = {}\n",
        "        new_para['id'] = para['id'] + \"_\" + \"0\"\n",
        "        new_para['context'] = sentences[0]\n",
        "        new_dataset.append(new_para)\n",
        "        for i in range(1, len(sentences)):\n",
        "            de_sent = decontextualize(create_input(sentences, i, \"\", \"\"))\n",
        "            new_para = {}\n",
        "            new_para['id'] = para['id'] + \"_\" + str(i)\n",
        "            new_para['context'] = de_sent\n",
        "            new_dataset.append(new_para)\n",
        "    to_save = {'data':new_dataset}\n",
        "    with open(save_path, 'w') as save_file:\n",
        "        json.dump(to_save, save_file, indent=4)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3cPOwwOMLvv",
        "outputId": "0ea99aa0-0acb-4bc6-c2a8-103031b4ad05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start 1.json\n",
            "Finish 1.json\n",
            "Start 2.json\n",
            "Finish 2.json\n",
            "Start 3.json\n",
            "Finish 3.json\n",
            "Start 4.json\n",
            "Finish 4.json\n",
            "Start 5.json\n",
            "Finish 5.json\n",
            "Start 6.json\n",
            "Finish 6.json\n",
            "Start 7.json\n",
            "Finish 7.json\n",
            "Start 8.json\n",
            "Finish 8.json\n",
            "Start 9.json\n",
            "Finish 9.json\n",
            "Start 10.json\n",
            "Finish 10.json\n",
            "Start 11.json\n",
            "Finish 11.json\n",
            "Start 12.json\n",
            "Finish 12.json\n",
            "Start 13.json\n",
            "Finish 13.json\n",
            "Start 14.json\n",
            "Finish 14.json\n",
            "Start 15.json\n",
            "Finish 15.json\n",
            "Start 16.json\n",
            "Finish 16.json\n",
            "Start 17.json\n",
            "Finish 17.json\n",
            "Start 18.json\n",
            "Finish 18.json\n",
            "Start 19.json\n",
            "Finish 19.json\n",
            "Start 20.json\n",
            "Finish 20.json\n"
          ]
        }
      ],
      "source": [
        "# Take around 100 mins using A100 from Google Colab\n",
        "original_dir = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/paragraph-dev-segment-original\"        # Path to context to decontextualize\n",
        "target_dir = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/paragraph-dev-segment-target\"            # Save path\n",
        "for i in range(1, 21):\n",
        "    file_name = str(i)+\".json\"          # Tracking progress\n",
        "    print(\"Start {}\".format(file_name))\n",
        "    original_path = os.path.join(original_dir, file_name)\n",
        "    target_path = os.path.join(target_dir, file_name)\n",
        "    segment_dataset(original_path, target_path)\n",
        "    print(\"Finish {}\".format(file_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B-jhqrlVR7r",
        "outputId": "f8c78baa-a07c-47cb-9c3a-2583b992f746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "def preprocess_file(file_path):\n",
        "    with open(file_path) as dataset_file:\n",
        "        dataset = json.load(dataset_file)['data']\n",
        "    for i, context in enumerate(dataset):\n",
        "        new_context = context['context'].split(\"####\")[-1].strip()\n",
        "        dataset[i] = {'id': context['id'], 'context': new_context}\n",
        "    return dataset\n",
        "\n",
        "target_dir = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/paragraph-dev-segment-target\"\n",
        "full_dataset = []\n",
        "for i in range(1, 21):\n",
        "    print(i)            # Tracking progress\n",
        "    file_name = str(i)+\".json\"\n",
        "    file_path = os.path.join(target_dir, file_name)\n",
        "    full_dataset.extend(preprocess_file(file_path))\n",
        "save_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/full_decontextualization.json\" # Path for saving full_decontextualization\n",
        "to_save = {'data': full_dataset}\n",
        "json_object = json.dumps(to_save, indent=4)\n",
        "with open(save_path, \"w\") as outfile:\n",
        "    outfile.write(json_object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Match the Decontextualized Sentences with questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PS3y8vLCX10"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Merge Decontextualization (Downloaded from Google Colab)\n",
        "\"\"\"\n",
        "def merge_context_list(decontextualization_path):\n",
        "    with open(decontextualization_path) as dataset_file:\n",
        "        dataset = json.load(dataset_file)['data']\n",
        "    all_context = {}\n",
        "    for context in dataset:\n",
        "        first, second = context['id'].split(\"_\")\n",
        "        if second == '0':\n",
        "            all_context[first] = {second:context['context']}\n",
        "        else:\n",
        "            all_context[first][second] = context['context']\n",
        "    return all_context\n",
        "\n",
        "def match_question_sentences(decontextualization_path, original_data_path, paragraph_path, save_path):\n",
        "    \"\"\"\n",
        "    INPUT:\n",
        "        decontextualization_path: \n",
        "        original_data_path: \n",
        "        paragraph_path: \n",
        "        save_path: \n",
        "    \"\"\"\n",
        "    context_dict = merge_context_list(decontextualization_path)\n",
        "    with open(original_data_path) as dataset_file:\n",
        "        dataset = json.load(dataset_file)['data']\n",
        "    with open(paragraph_path) as paragraph_file:\n",
        "        paragraphs = json.load(paragraph_file)['data']\n",
        "    \n",
        "    new_dataset = []\n",
        "    count_pass = 0                  # If there is any failure to match question with sentences\n",
        "    for question in dataset:\n",
        "        paragraph_id = 'not_found'\n",
        "        for p in paragraphs:\n",
        "            if p['context'] == question['context']:\n",
        "                paragraph_id = p['id']\n",
        "        if paragraph_id == 'not_found':\n",
        "            count_pass += 1\n",
        "            continue\n",
        "        p_dict = context_dict[paragraph_id]\n",
        "        for key in p_dict:\n",
        "            new_question = {}\n",
        "            new_question['question'] = question['question']\n",
        "            new_question['id'] = question['id'] + \"_\" + key\n",
        "            new_question['context'] = p_dict[key]\n",
        "            new_question['answers'] = {'answer_start':[], 'text':[]}\n",
        "            new_dataset.append(new_question)\n",
        "    print(\"Skip {} questions\".format(count_pass))\n",
        "\n",
        "    # Save the result\n",
        "    to_save = {'data': new_dataset}\n",
        "    json_object = json.dumps(to_save, indent=4)\n",
        "    with open(save_path, \"w\") as outfile:\n",
        "        outfile.write(json_object)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Match the biased dev set\n",
        "decontextualization_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/full_decontextualization.json\"\n",
        "original_data_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/Shortcut/dev/answer_position.json\"\n",
        "paragraph_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/paragraph-dev-v1.1.json\"\n",
        "save_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/Shortcut/dev/decontextualized_position.json\"\n",
        "match_question_sentences(decontextualization_path, original_data_path, paragraph_path, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Match the anti-biased dev set\n",
        "decontextualization_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/full_decontextualization.json\"\n",
        "original_data_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/Shortcut/dev/anti_answer_position.json\"\n",
        "paragraph_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/paragraph-dev-v1.1.json\"\n",
        "save_path = \"/Volumes/Share/tran_s2/AgainstShortcut/debias_experiment/Data/Shortcut/dev/decontextualized_anti_position.json\"\n",
        "match_question_sentences(decontextualization_path, original_data_path, paragraph_path, save_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
